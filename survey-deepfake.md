

结论: 当前造假的主要方法和技术手段主要还是以Deepfake换脸和AIGC合成为主, Deepfake换脸的方法主要处理视频模态, 也可以处理图片模态, 技术依赖于传统的GAN-based的生成模型, 方法比较老(2-3年前). 而AIGC生成一般是直接根据参考人物图片生成新的图片或者是数字人视频, 主要是基于DIffusion-based的方法.



|  模型名称  | 输入输出                                                     | 论文标题                                                     |  会议/期刊   | 开源链接                                                     | 核心方法与技术亮点                                           | 基础架构                         |
| :--------: | :----------------------------------------------------------- | :----------------------------------------------------------- | :----------: | :----------------------------------------------------------- | :----------------------------------------------------------- | -------------------------------- |
|  SimSwap   | 换脸，输入原图像、目标图像或视频，输出换脸后的图像或视频。   | SimSwap: An Efficient Framework For High Fidelity Face Swapping | ACMMM  2020  | 开源，[GitHub - neuralchen/SimSwap: An arbitrary face-swapping framework on images and videos with one single trained model!](https://github.com/neuralchen/SimSwap) | 特点在于特征级身份注入：替换目标特征中的身份信息，同时通过损失函数保留目标图像属性。 | GAN-based                        |
|    Roop    | 换脸，输入原图像、目标图像或视频，一键完成换脸效果。         | 非学术                                                       |      ——      | 开源，[GitHub - s0md3v/roop: one-click face swap](https://github.com/s0md3v/roop) | 简单好用门槛低，无需训练一键式换脸。项目已停止更新。         | 基于工程整合封装了多个预训练模型 |
|  InfoSwap  | 换脸，输入输入原图像和目标图像，输出换脸后的图像。           | InfoSwap: Information Bottleneck Disentanglement for Identity Swapping |  CVPR  2021  | 开源，[GitHub - GGGHSL/InfoSwap-master: Official PyTorch Implementation for InfoSwap](https://github.com/GGGHSL/InfoSwap-master) | 特点在于信息瓶颈解耦：通过信息瓶颈原理，明确地将身份信息与属性信息在特征空间中分离，实现纯粹的身份交换。 | GAN-based                        |
| StyleSwap  | 换脸，输入输入原图像、目标图像或风格编码，输出换脸后的图像。 | StyleSwap: Style-Based Generator Empowers Robust Face Swapping |  ECCV  2022  | 开源，[GitHub - Seanseattle/StyleSwap: StyleSwap: Style-Based Generator Empowers Robust Face Swapping (ECCV 2022)](https://github.com/Seanseattle/StyleSwap) | 该论文提出一种基于风格生成器（StyleGAN2）的人脸换脸框架，利用最小改动的StyleGAN2架构及换脸驱动的掩码分支，实现源脸身份与目标脸环境信息的有效融合。同时设计了换脸引导的身份反演策略以优化身份相似度。 | GAN-based                        |
| PhotoMaker | 输入少量图片和文本描述姿势、背景等，输出对应的全新的人物图片。 | PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding |  CVPR  2024  | 开源，[GitHub - TencentARC/PhotoMaker: PhotoMaker [CVPR 2024\]](https://github.com/TencentARC/PhotoMaker?tab=readme-ov-file) | 核心方法是堆叠式身份嵌入。PhotoMaker使用 CLIP 图像编码器 + 类别融合得到每张 ID 嵌入，拼接堆叠形成统一的 identity condition后，在 U-Net 的各个cross-attention 层注入，实现高保真的人脸图像生成。 | Diffusion-based                  |
| StoryMaker | 输入多段文本和图片，输出一个图片序列。                       | StoryMaker: Towards Holistic Consistent Characters in Text-to-image Generation | arXiv   2023 | 开源，[GitHub - RedAIGC/StoryMaker: StoryMaker: Towards consistent characters in text-to-image generation](https://github.com/RedAIGC/StoryMaker) | 核心方法是位置感知的堆叠式角色嵌入。StoryMaker 使用 ArcFace 面部编码器与CLIP 角色图像编码器提取每个角色的面部与整体外观特征，通过 Positional-aware Perceiver Resampler 进行融合，形成带空间位置信息的角色嵌入表示。再将这些嵌入堆叠注入到 U-Net 的各个 cross-attention 层。可实现多角色场景中面部、服饰、身体姿态等各方面统一的图像生成。 | Diffusion-based                  |
|   PuLID    | 输入参考图像和文本描述，输出包含参考身份的人脸，并按照文本描述生成完整图像。 | PuLID: Pure and Lightning ID Customization via Contrastive Alignment | NeurIPS 2024 | 开源，[GitHub - ToTheBeginning/PuLID: [NeurIPS 2024\] Official code for PuLID: Pure and Lightning ID Customization via Contrastive Alignment](https://github.com/ToTheBeginning/PuLID) | 核心方法是对比对齐驱动的身份注入。PuLID 使用 ArcFace 和 DINO v2 提取身份特征，通过对比对齐损失和精确身份损失增强身份一致性。将提取的身份嵌入堆叠后注入 U-Net 的 cross-attention 层，与文本提示协同控制生成过程。PuLID生成的图像在准确呈现指定身份的同时，很好地保持了插入身份前后图像元素的一致性。 | Diffusion-based                  |
| StyleTalk  | 输入音频和图像，输出风格化说话视频。                         | StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles |  AAAI 2023   | 开源，[GitHub - FuxiVirtualHuman/styletalk](https://github.com/FuxiVirtualHuman/styletalk) | 该论文提出了一种用于单图像驱动的说话人头部生成的新框架，名为 StyleTalk。StyleTalk 的核心创新在于引入了Style Vector机制，可对生成视频中的语速、语调、情感状态，如高兴、愤怒、平静等说话风格维度进行显式控制。 | Diffusion-based                  |
| MultiTalk  | 输入多段音频，可选择输入任务图像，输出多人参与的对话视频。   | Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation |  arXiv 2025  | 开源，[GitHub - MeiGen-AI/MultiTalk: Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation](https://github.com/meigen-ai/multitalk) | 该论文提出了一种用于多人对话视频生成的新框架，通过引入 Label Rotary Position Embedding，将多路音频流与对应说话人精确绑定，有效实现多角色嘴型同步。MultiTalk 在训练过程中采用部分参数微调和多任务训练策略，保留了保留原模型的指令响应能力。该论文表示 MultiTalk 在多个数据集比现有其他方法更加出色。 | Diffusion-based                  |
|            |                                                              |                                                              |              |                                                              |                                                              |                                  |

